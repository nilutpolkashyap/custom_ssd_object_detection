{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python starter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV:  4.5.3\n",
      "Numpy:  1.19.5\n"
     ]
    }
   ],
   "source": [
    "from starter import *\n",
    "library_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arduino', 'microbit', 'esp32']\n"
     ]
    }
   ],
   "source": [
    "labels = ['arduino', 'microbit', 'esp32']\n",
    "# labels = ['arduino']\n",
    "number_imgs = 150\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.join('..', 'ssd_workspace', 'image_dataset', 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(IMAGES_PATH):\n",
    "    !mkdir {IMAGES_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..\\\\ssd_workspace\\\\videos\\\\arduino.mp4', '..\\\\ssd_workspace\\\\videos\\\\microbit.mp4', '..\\\\ssd_workspace\\\\videos\\\\esp32.mp4']\n"
     ]
    }
   ],
   "source": [
    "VIDEOS_PATH = []\n",
    "VIDEOS_PATH.append(os.path.join('..', 'ssd_workspace','videos', 'arduino.mp4'))\n",
    "VIDEOS_PATH.append(os.path.join('..', 'ssd_workspace','videos','microbit.mp4'))\n",
    "VIDEOS_PATH.append(os.path.join('..', 'ssd_workspace','videos','esp32.mp4'))\n",
    "print(VIDEOS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arduino': '..\\\\ssd_workspace\\\\videos\\\\arduino.mp4', 'microbit': '..\\\\ssd_workspace\\\\videos\\\\microbit.mp4', 'esp32': '..\\\\ssd_workspace\\\\videos\\\\esp32.mp4'}\n"
     ]
    }
   ],
   "source": [
    "videos = {}\n",
    "for key in labels:\n",
    "    for value in VIDEOS_PATH:\n",
    "        videos[key] = value\n",
    "        VIDEOS_PATH.remove(value)\n",
    "        break\n",
    "print(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting images for arduino\n",
      "Number of images collected for arduino : 25\n",
      "Collecting images for microbit\n",
      "Number of images collected for microbit : 25\n",
      "Collecting images for esp32\n",
      "Number of images collected for esp32 : 25\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    print(\"Collecting images for {}\".format(label))\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(videos[label]) \n",
    "\n",
    "        img_num = 0\n",
    "        while(cap.isOpened()):  \n",
    "            ret,frame = cap.read()\n",
    "\n",
    "            cv2.imshow('video',frame)\n",
    "            \"\"\"\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):      #To check if 'q' key is pressed \n",
    "                break\n",
    "\n",
    "            if cv2.waitKey(33) == ord('a'):\n",
    "                print(\"pressed a\")    \"\"\"\n",
    "            if img_num == 25:\n",
    "                print(\"Number of images collected for {} : {}\".format(label, img_num))\n",
    "                break\n",
    "\n",
    "            k = cv2.waitKey(1)\n",
    "            if k & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            if k & 0xFF == ord('a'):\n",
    "                img_name = os.path.join(IMAGES_PATH, label+'_'+'{}.jpg'.format(img_num))\n",
    "                cv2.imwrite(img_name, frame)\n",
    "                img_num = img_num + 1\n",
    "    #             print(\"image saved {}\".format(img_num))\n",
    "\n",
    "            time.sleep(0.05)\n",
    "    except cv2.error as error:\n",
    "        print(\"Video ended\")\n",
    "#         print(\"[Error]: {}\".format(error))\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "current_dir = IMAGES_PATH\n",
    "split_pct = 10;\n",
    "# train_set = os.path.join('..', 'ssd_workspace', 'image_dataset', 'train_test_set')\n",
    "# test_set = os.path.join('..', 'ssd_workspace', 'image_dataset', 'train_test_set')\n",
    "file_train = open(\"train_test_set/train.txt\", \"w\")  \n",
    "file_test = open(\"train_test_set/test.txt\", \"w\")  \n",
    "counter = 1  \n",
    "index_test = round(100 / split_pct)  \n",
    "for pathAndFilename in glob.iglob(os.path.join(current_dir, \"*.jpg\")):  \n",
    "        title, ext = os.path.splitext(os.path.basename(pathAndFilename))\n",
    "        if counter == index_test:\n",
    "                counter = 1\n",
    "                file_test.write(current_dir + \"/\" + title + '.jpg' + \"\\n\")\n",
    "        else:\n",
    "                file_train.write(current_dir + \"/\" + title + '.jpg' + \"\\n\")\n",
    "                counter = counter + 1\n",
    "file_train.close()\n",
    "file_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting image1 / 75 \\ n arduino_0\n",
      "converting image2 / 75 \\ n arduino_1\n",
      "converting image3 / 75 \\ n arduino_10\n",
      "converting image4 / 75 \\ n arduino_11\n",
      "converting image5 / 75 \\ n arduino_12\n",
      "converting image6 / 75 \\ n arduino_13\n",
      "converting image7 / 75 \\ n arduino_14\n",
      "converting image8 / 75 \\ n arduino_15\n",
      "converting image9 / 75 \\ n arduino_16\n",
      "converting image10 / 75 \\ n arduino_17\n",
      "converting image11 / 75 \\ n arduino_18\n",
      "converting image12 / 75 \\ n arduino_19\n",
      "converting image13 / 75 \\ n arduino_2\n",
      "converting image14 / 75 \\ n arduino_20\n",
      "converting image15 / 75 \\ n arduino_21\n",
      "converting image16 / 75 \\ n arduino_22\n",
      "converting image17 / 75 \\ n arduino_23\n",
      "converting image18 / 75 \\ n arduino_24\n",
      "converting image19 / 75 \\ n arduino_3\n",
      "converting image20 / 75 \\ n arduino_4\n",
      "converting image21 / 75 \\ n arduino_5\n",
      "converting image22 / 75 \\ n arduino_6\n",
      "converting image23 / 75 \\ n arduino_7\n",
      "converting image24 / 75 \\ n arduino_8\n",
      "converting image25 / 75 \\ n arduino_9\n",
      "converting image26 / 75 \\ n esp32_0\n",
      "converting image27 / 75 \\ n esp32_1\n",
      "converting image28 / 75 \\ n esp32_10\n",
      "converting image29 / 75 \\ n esp32_11\n",
      "converting image30 / 75 \\ n esp32_12\n",
      "converting image31 / 75 \\ n esp32_13\n",
      "converting image32 / 75 \\ n esp32_14\n",
      "converting image33 / 75 \\ n esp32_15\n",
      "converting image34 / 75 \\ n esp32_16\n",
      "converting image35 / 75 \\ n esp32_17\n",
      "converting image36 / 75 \\ n esp32_18\n",
      "converting image37 / 75 \\ n esp32_19\n",
      "converting image38 / 75 \\ n esp32_2\n",
      "converting image39 / 75 \\ n esp32_20\n",
      "converting image40 / 75 \\ n esp32_21\n",
      "converting image41 / 75 \\ n esp32_22\n",
      "converting image42 / 75 \\ n esp32_23\n",
      "converting image43 / 75 \\ n esp32_24\n",
      "converting image44 / 75 \\ n esp32_3\n",
      "converting image45 / 75 \\ n esp32_4\n",
      "converting image46 / 75 \\ n esp32_5\n",
      "converting image47 / 75 \\ n esp32_6\n",
      "converting image48 / 75 \\ n esp32_7\n",
      "converting image49 / 75 \\ n esp32_8\n",
      "converting image50 / 75 \\ n esp32_9\n",
      "converting image51 / 75 \\ n microbit_0\n",
      "converting image52 / 75 \\ n microbit_1\n",
      "converting image53 / 75 \\ n microbit_10\n",
      "converting image54 / 75 \\ n microbit_11\n",
      "converting image55 / 75 \\ n microbit_12\n",
      "converting image56 / 75 \\ n microbit_13\n",
      "converting image57 / 75 \\ n microbit_14\n",
      "converting image58 / 75 \\ n microbit_15\n",
      "converting image59 / 75 \\ n microbit_16\n",
      "converting image60 / 75 \\ n microbit_17\n",
      "converting image61 / 75 \\ n microbit_18\n",
      "converting image62 / 75 \\ n microbit_19\n",
      "converting image63 / 75 \\ n microbit_2\n",
      "converting image64 / 75 \\ n microbit_20\n",
      "converting image65 / 75 \\ n microbit_21\n",
      "converting image66 / 75 \\ n microbit_22\n",
      "converting image67 / 75 \\ n microbit_23\n",
      "converting image68 / 75 \\ n microbit_24\n",
      "converting image69 / 75 \\ n microbit_3\n",
      "converting image70 / 75 \\ n microbit_4\n",
      "converting image71 / 75 \\ n microbit_5\n",
      "converting image72 / 75 \\ n microbit_6\n",
      "converting image73 / 75 \\ n microbit_7\n",
      "converting image74 / 75 \\ n microbit_8\n",
      "converting image75 / 75 \\ n microbit_9\n",
      "\n",
      "Finished converting the Pascal VOC dataset!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\tf_object_detection\\tf-gpu-venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#import xml.etree.elementtree as et\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "#There are only two definitions of my tags, depending on my own pictures\n",
    "VOC_LABELS = {\n",
    "    'none': (0, 'Background'),\n",
    "    'arduino': (1, 'arduino'),\n",
    "    'microbit': (2, 'microbit'),\n",
    "    'esp32': (2, 'esp32'),\n",
    "}\n",
    "\n",
    "#A folder for pictures and labels\n",
    "DIRECTORY_ANNOTATIONS = 'F:/github/ssd_workspace/image_dataset/annotations/'\n",
    "DIRECTORY_IMAGES = 'F:/github/ssd_workspace/image_dataset/images/'\n",
    "\n",
    "#Random seed\n",
    "RANDOM_SEED = 4242\n",
    "SAMPLES_PER_FILES = 1 #samples per file\n",
    "\n",
    "\n",
    "#Generate integer, floating point, and string properties\n",
    "def int64_feature(value):\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def float_feature(value):\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    if not isinstance(value, list):\n",
    "        value = [value]\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "#Image processing\n",
    "def _process_image(directory, name):\n",
    "    # Read the image file.\n",
    "    filename = directory + DIRECTORY_IMAGES + name + '.jpg'\n",
    "    image_data = tf.io.gfile.GFile(filename, 'rb').read()\n",
    "\n",
    "    # Read the XML annotation file.\n",
    "    filename = os.path.join(directory, DIRECTORY_ANNOTATIONS, name + '.xml')\n",
    "    tree = et.parse(filename)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Image shape.\n",
    "    size = root.find('size')\n",
    "    shape = [int(size.find('height').text),\n",
    "             int(size.find('width').text),\n",
    "             int(size.find('depth').text)]\n",
    "    # Find annotations.\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    labels_text = []\n",
    "    difficult = []\n",
    "    truncated = []\n",
    "    for obj in root.findall('object'):\n",
    "        label = obj.find('name').text\n",
    "        labels.append(int(VOC_LABELS[label][0]))\n",
    "        labels_text.append (label.encode ('ascii')) #ාchange to ASCII format\n",
    "\n",
    "        if obj.find('difficult'):\n",
    "            difficult.append(int(obj.find('difficult').text))\n",
    "        else:\n",
    "            difficult.append(0)\n",
    "        if obj.find('truncated'):\n",
    "            truncated.append(int(obj.find('truncated').text))\n",
    "        else:\n",
    "            truncated.append(0)\n",
    "\n",
    "        bbox = obj.find('bndbox')\n",
    "\n",
    "        ####################################################################\t\t\n",
    "        bboxes.append((max(float(bbox.find('ymin').text) / shape[0],0.0),\n",
    "                   max(float(bbox.find('xmin').text) / shape[1],0.0),\n",
    "                   min(float(bbox.find('ymax').text) / shape[0],1.0),\n",
    "                   min(float(bbox.find('xmax').text) / shape[1],1.0)\n",
    "                   ))\n",
    "        ######################################################################\n",
    "        # the above code in ### replaced by below commented code\n",
    "        # a = float(bbox.find('ymin').text) / shape[0]\n",
    "        # b = float(bbox.find('xmin').text) / shape[1]\n",
    "        # a1 = float(bbox.find('ymax').text) / shape[0]\n",
    "        # b1 = float(bbox.find('xmax').text) / shape[1]\n",
    "        # a_e = a1 - a\n",
    "        # b_e = b1 - b\n",
    "        # if abs(a_e) < 1 and abs(b_e) < 1:\n",
    "            # bboxes.append((a, b, a1, b1))\n",
    "\n",
    "    return image_data, shape, bboxes, labels, labels_text, difficult, truncated\n",
    "\n",
    "\n",
    "#Conversion example\n",
    "def _convert_to_example(image_data, labels, labels_text, bboxes, shape,\n",
    "                        difficult, truncated):\n",
    "    xmin = []\n",
    "    ymin = []\n",
    "    xmax = []\n",
    "    ymax = []\n",
    "    for b in bboxes:\n",
    "        assert len(b) == 4\n",
    "        # pylint: disable=expression-not-assigned\n",
    "        [l.append(point) for l, point in zip([ymin, xmin, ymax, xmax], b)]\n",
    "        # pylint: enable=expression-not-assigned\n",
    "\n",
    "    image_format = b'JPEG'\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': int64_feature(shape[0]),\n",
    "        'image/width': int64_feature(shape[1]),\n",
    "        'image/channels': int64_feature(shape[2]),\n",
    "        'image/shape': int64_feature(shape),\n",
    "        'image/object/bbox/xmin': float_feature(xmin),\n",
    "        'image/object/bbox/xmax': float_feature(xmax),\n",
    "        'image/object/bbox/ymin': float_feature(ymin),\n",
    "        'image/object/bbox/ymax': float_feature(ymax),\n",
    "        'image/object/bbox/label': int64_feature(labels),\n",
    "        'image/object/bbox/label_text': bytes_feature(labels_text),\n",
    "        'image/object/bbox/difficult': int64_feature(difficult),\n",
    "        'image/object/bbox/truncated': int64_feature(truncated),\n",
    "        'image/format': bytes_feature(image_format),\n",
    "        'image/encoded': bytes_feature(image_data)}))\n",
    "    return example\n",
    "\n",
    "\n",
    "#Add to tfrecord\n",
    "def _add_to_tfrecord(dataset_dir, name, tfrecord_writer):\n",
    "    image_data, shape, bboxes, labels, labels_text, difficult, truncated = \\\n",
    "        _process_image(dataset_dir, name)\n",
    "    example = _convert_to_example(image_data, labels, labels_text,\n",
    "                                  bboxes, shape, difficult, truncated)\n",
    "    tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "\n",
    "#Name is the prefix of the converted file\n",
    "def _get_output_filename(output_dir, name, idx):\n",
    "    return '%s/%s_%03d.tfrecord' % (output_dir, name, idx)\n",
    "\n",
    "\n",
    "def run(dataset_dir, output_dir, name='voc_train', shuffling=False):\n",
    "    if not tf.io.gfile.exists(dataset_dir):\n",
    "        tf.io.gfile.makedirs(dataset_dir)\n",
    "\n",
    "    path = os.path.join(dataset_dir, DIRECTORY_ANNOTATIONS)\n",
    "    filenames = sorted (os.listdir (path)) #(sort)\n",
    "    if shuffling:\n",
    "        random.seed(RANDOM_SEED)\n",
    "        random.shuffle(filenames)\n",
    "\n",
    "    i = 0\n",
    "    fidx = 0\n",
    "    while i < len(filenames):\n",
    "        # Open new TFRecord file.\n",
    "        tf_filename = _get_output_filename(output_dir, name, fidx)\n",
    "        with tf.io.TFRecordWriter(tf_filename) as tfrecord_writer:\n",
    "            j = 0\n",
    "            while i < len(filenames) and j < SAMPLES_PER_FILES:\n",
    "                sys.stdout.write ('converting image%d /% d \\ n '% (i + 1, len (filenames))) #(terminal printing, similar to print)\n",
    "                sys.stdout.flush() #(buffer)\n",
    "\n",
    "                filename = filenames[i]\n",
    "                img_name = filename[:-4]\n",
    "                print(img_name)\n",
    "                _add_to_tfrecord(dataset_dir, img_name, tfrecord_writer)\n",
    "                i += 1\n",
    "                j += 1\n",
    "            fidx += 1\n",
    "\n",
    "    print('\\nFinished converting the Pascal VOC dataset!')\n",
    "\n",
    "\n",
    "#Original dataset path, output path and output file name\n",
    "dataset_dir = \"\"   #\"/voc2007/\"\n",
    "output_dir = \"../ssd_workspace/tfrecords/\"\n",
    "name = \"voc_2007_train\"\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    run(dataset_dir, output_dir, name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.compat.v1.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015 Paul Balanca. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Provides data for the Pascal VOC Dataset (images + annotations).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Modify the entire file according to your own training data\n",
    "import tensorflow as tf\n",
    "from datasets import pascalvoc_common\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "FILE_PATTERN = 'voc_2007_%s_*.tfrecord'\n",
    "ITEMS_TO_DESCRIPTIONS = {\n",
    "    'image': 'A color image of varying height and width.',\n",
    "    'shape': 'Shape of the image',\n",
    "    'object/bbox': 'A list of bounding boxes, one per each object.',\n",
    "    'object/label': 'A list of labels, one per each object.',\n",
    "}\n",
    "\n",
    "# (Images, Objects) statistics on every class.\n",
    "TRAIN_STATISTICS = {\n",
    "    'none': (0, 0),\n",
    "    'arduino': (23, 23),\n",
    "    'microbit': (23, 23),\n",
    "    'esp32': (22, 22),\n",
    "\t}\n",
    "\t\n",
    "\t\n",
    "TEST_STATISTICS = {\n",
    "    'none': (0, 0),\n",
    "    'arduino': (2, 2),\n",
    "    'microbit': (2, 2),\n",
    "    'esp32': (3, 3),\n",
    "\t}\t\n",
    "\n",
    "\t\n",
    "\t\n",
    "SPLITS_TO_SIZES = {\n",
    "    'train': 68,  #Training data volume\n",
    "    'test': 7,   #Test data volume\n",
    "}\n",
    "SPLITS_TO_STATISTICS = {\n",
    "    'train': TRAIN_STATISTICS,\n",
    "    'test': TEST_STATISTICS,\n",
    "}\n",
    "NUM_CLASSES = 3 #modify according to the actual category of your own data (without background)\n",
    "\n",
    "\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n",
    "    \"\"\"Gets a dataset tuple with instructions for reading ImageNet.\n",
    "    Args:\n",
    "      split_name: A train/test split name.\n",
    "      dataset_dir: The base directory of the dataset sources.\n",
    "      file_pattern: The file pattern to use when matching the dataset sources.\n",
    "        It is assumed that the pattern contains a '%s' string so that the split\n",
    "        name can be inserted.\n",
    "      reader: The TensorFlow reader type.\n",
    "    Returns:\n",
    "      A `Dataset` namedtuple.\n",
    "    Raises:\n",
    "        ValueError: if `split_name` is not a valid train/test split.\n",
    "    \"\"\"\n",
    "    if not file_pattern:\n",
    "        file_pattern = FILE_PATTERN\n",
    "    return pascalvoc_common.get_split(split_name, dataset_dir,\n",
    "                                      file_pattern, reader,\n",
    "                                      SPLITS_TO_SIZES,\n",
    "                                      ITEMS_TO_DESCRIPTIONS,\n",
    "                                      NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015 Paul Balanca. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Provides data for the Pascal VOC Dataset (images + annotations).\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "#import dataset_utils\n",
    "from datasets import dataset_utils\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "VOC_LABELS = {\n",
    "    'none': (0, 'Background'),\n",
    "    'arduino': (1, 'arduino'),\n",
    "    'microbit': (2, 'microbit'),\n",
    "    'esp32' : (3, 'esp32'),\n",
    "}\n",
    "\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern, reader,\n",
    "              split_to_sizes, items_to_descriptions, num_classes):\n",
    "    \"\"\"Gets a dataset tuple with instructions for reading Pascal VOC dataset.\n",
    "    Args:\n",
    "      split_name: A train/test split name.\n",
    "      dataset_dir: The base directory of the dataset sources.\n",
    "      file_pattern: The file pattern to use when matching the dataset sources.\n",
    "        It is assumed that the pattern contains a '%s' string so that the split\n",
    "        name can be inserted.\n",
    "      reader: The TensorFlow reader type.\n",
    "    Returns:\n",
    "      A `Dataset` namedtuple.\n",
    "    Raises:\n",
    "        ValueError: if `split_name` is not a valid train/test split.\n",
    "    \"\"\"\n",
    "    if split_name not in split_to_sizes:\n",
    "        raise ValueError('split name %s was not recognized.' % split_name)\n",
    "    file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n",
    "\n",
    "    # Allowing None in the signature so that dataset_factory can use the default.\n",
    "    if reader is None:\n",
    "        reader = tf.TFRecordReader\n",
    "    # Features in Pascal VOC TFRecords.\n",
    "    keys_to_features = {\n",
    "        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),\n",
    "        'image/height': tf.FixedLenFeature([1], tf.int64),\n",
    "        'image/width': tf.FixedLenFeature([1], tf.int64),\n",
    "        'image/channels': tf.FixedLenFeature([1], tf.int64),\n",
    "        'image/shape': tf.FixedLenFeature([3], tf.int64),\n",
    "        'image/object/bbox/xmin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xmax': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymax': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/label': tf.VarLenFeature(dtype=tf.int64),\n",
    "        'image/object/bbox/difficult': tf.VarLenFeature(dtype=tf.int64),\n",
    "        'image/object/bbox/truncated': tf.VarLenFeature(dtype=tf.int64),\n",
    "    }\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image('image/encoded', 'image/format'),\n",
    "        'shape': slim.tfexample_decoder.Tensor('image/shape'),\n",
    "        'object/bbox': slim.tfexample_decoder.BoundingBox(\n",
    "                ['ymin', 'xmin', 'ymax', 'xmax'], 'image/object/bbox/'),\n",
    "        'object/label': slim.tfexample_decoder.Tensor('image/object/bbox/label'),\n",
    "        'object/difficult': slim.tfexample_decoder.Tensor('image/object/bbox/difficult'),\n",
    "        'object/truncated': slim.tfexample_decoder.Tensor('image/object/bbox/truncated'),\n",
    "    }\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(\n",
    "        keys_to_features, items_to_handlers)\n",
    "\n",
    "    labels_to_names = None\n",
    "    if dataset_utils.has_labels(dataset_dir):\n",
    "        labels_to_names = dataset_utils.read_label_file(dataset_dir)\n",
    "    # else:\n",
    "    #     labels_to_names = create_readable_names_for_imagenet_labels()\n",
    "    #     dataset_utils.write_label_file(labels_to_names, dataset_dir)\n",
    "\n",
    "    return slim.dataset.Dataset(\n",
    "            data_sources=file_pattern,\n",
    "            reader=reader,\n",
    "            decoder=decoder,\n",
    "            num_samples=split_to_sizes[split_name],\n",
    "            items_to_descriptions=items_to_descriptions,\n",
    "            num_classes=num_classes,\n",
    "            labels_to_names=labels_to_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\tf_object_detection\\tf-gpu-venv\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'training' from 'tensorflow' (F:\\tf_object_detection\\tf-gpu-venv\\lib\\site-packages\\tensorflow\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1604/919682442.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdataset_factory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdeployment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_deploy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnets_factory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing_factory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\tf_object_detection\\tf-gpu-venv\\lib\\site-packages\\object_detection-0.1-py3.7.egg\\nets\\nets_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmobilenet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmobilenet_v2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmobilenet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmobilenet_v3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnasnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnasnet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnasnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpnasnet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\tf_object_detection\\tf-gpu-venv\\lib\\site-packages\\object_detection-0.1-py3.7.egg\\nets\\nasnet\\nasnet.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_slim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mslim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcontrib_training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnasnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnasnet_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'training' from 'tensorflow' (F:\\tf_object_detection\\tf-gpu-venv\\lib\\site-packages\\tensorflow\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "\n",
    "from datasets import dataset_factory\n",
    "from deployment import model_deploy\n",
    "from nets import nets_factory\n",
    "from preprocessing import preprocessing_factory\n",
    "import tf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1604/2102742336.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mslim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mDATA_FORMAT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'NCHW'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mDATASET_DIR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/tfrecord'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "DATA_FORMAT = 'NCHW'\n",
    "\n",
    "DATASET_DIR='/tfrecord'\n",
    "TRAIN_DIR='D:/env_conda_1.14_for_SSD/SSD_CUSTOM/log/'\n",
    "CHECKPOINT_PATH='checkpoints/ssd_300_vgg.ckpt'\n",
    "\n",
    "# =========================================================================== #\n",
    "# SSD Network flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'loss_alpha', 1., 'Alpha parameter in the loss function.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'negative_ratio', 3., 'Negative ratio in the loss function.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'match_threshold', 0.5, 'Matching threshold in the loss function.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# General Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'train_dir', 'D:/env_conda_1.14_for_SSD/SSD_CUSTOM/log/',\n",
    "    'Directory where checkpoints and event logs are written to.')\n",
    "tf.app.flags.DEFINE_integer('num_clones', 1,\n",
    "                            'Number of model clones to deploy.')\n",
    "tf.app.flags.DEFINE_boolean('clone_on_cpu', False,\n",
    "                            'Use CPUs to deploy clones.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_readers', 4,\n",
    "    'The number of parallel readers that read data from the dataset.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_preprocessing_threads', 4,\n",
    "    'The number of threads used to create the batches.')\n",
    "\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'log_every_n_steps', 10,\n",
    "    'The frequency with which logs are print.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_summaries_secs', 600,\n",
    "    'The frequency with which summaries are saved, in seconds.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'save_interval_secs', 600,\n",
    "    'The frequency with which the model is saved, in seconds.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'gpu_memory_fraction', 0.8, 'GPU memory fraction to use.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Optimization Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'weight_decay', 0.00004, 'The weight decay on the model weights.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'optimizer', 'rmsprop',\n",
    "    'The name of the optimizer, one of \"adadelta\", \"adagrad\", \"adam\",'\n",
    "    '\"ftrl\", \"momentum\", \"sgd\" or \"rmsprop\".')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adadelta_rho', 0.95,\n",
    "    'The decay rate for adadelta.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adagrad_initial_accumulator_value', 0.1,\n",
    "    'Starting value for the AdaGrad accumulators.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adam_beta1', 0.9,\n",
    "    'The exponential decay rate for the 1st moment estimates.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'adam_beta2', 0.999,\n",
    "    'The exponential decay rate for the 2nd moment estimates.')\n",
    "tf.app.flags.DEFINE_float('opt_epsilon', 1.0, 'Epsilon term for the optimizer.')\n",
    "tf.app.flags.DEFINE_float('ftrl_learning_rate_power', -0.5,\n",
    "                          'The learning rate power.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_initial_accumulator_value', 0.1,\n",
    "    'Starting value for the FTRL accumulators.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_l1', 0.0, 'The FTRL l1 regularization strength.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'ftrl_l2', 0.0, 'The FTRL l2 regularization strength.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'momentum', 0.9,\n",
    "    'The momentum for the MomentumOptimizer and RMSPropOptimizer.')\n",
    "tf.app.flags.DEFINE_float('rmsprop_momentum', 0.9, 'Momentum.')\n",
    "tf.app.flags.DEFINE_float('rmsprop_decay', 0.9, 'Decay term for RMSProp.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Learning Rate Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'learning_rate_decay_type',\n",
    "    'exponential',\n",
    "    'Specifies how the learning rate is decayed. One of \"fixed\", \"exponential\",'\n",
    "    ' or \"polynomial\"')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'end_learning_rate', 0.0001,\n",
    "    'The minimal end learning rate used by a polynomial decay learning rate.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'label_smoothing', 0.0, 'The amount of label smoothing.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'learning_rate_decay_factor', 0.94, 'Learning rate decay factor.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'num_epochs_per_decay', 2.0,\n",
    "    'Number of epochs after which learning rate decays.')\n",
    "tf.app.flags.DEFINE_float(\n",
    "    'moving_average_decay', None,\n",
    "    'The decay to use for the moving average.'\n",
    "    'If left as None, then moving averages are not used.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Dataset Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_name', 'pascalvoc_2007', 'The name of the dataset to load.')   #pascalvoc_2007\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'num_classes', 2, 'Number of classes to use in the dataset.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_split_name', 'train', 'The name of the train/test split.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'dataset_dir', 'tfrecords', 'The directory where the dataset files are stored.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'labels_offset', 0,\n",
    "    'An offset for the labels in the dataset. This flag is primarily used to '\n",
    "    'evaluate the VGG and ResNet architectures which do not use a background '\n",
    "    'class for the ImageNet dataset.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'model_name', 'ssd_300_vgg', 'The name of the architecture to train.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'preprocessing_name', None, 'The name of the preprocessing to use. If left '\n",
    "    'as `None`, then the model_name flag is used.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'batch_size', 6, 'The number of samples in each batch.')\n",
    "tf.app.flags.DEFINE_integer(\n",
    "    'train_image_size', None, 'Train image size')\n",
    "tf.app.flags.DEFINE_integer('max_number_of_steps', None,\n",
    "                            'The maximum number of training steps.')\n",
    "\n",
    "# =========================================================================== #\n",
    "# Fine-Tuning Flags.\n",
    "# =========================================================================== #\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_path', 'checkpoints/ssd_300_vgg.ckpt/ssd_300_vgg.ckpt',\n",
    "    'The path to a checkpoint from which to fine-tune.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_model_scope', None,\n",
    "    'Model scope in the checkpoint. None if the same as the trained model.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'checkpoint_exclude_scopes', None,\n",
    "    'Comma-separated list of scopes of variables to exclude when restoring '\n",
    "    'from a checkpoint.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    'trainable_scopes', None,\n",
    "    'Comma-separated list of scopes to filter the set of variables to train.'\n",
    "    'By default, None would train all the variables.')\n",
    "tf.app.flags.DEFINE_boolean(\n",
    "    'ignore_missing_vars', False,\n",
    "    'When restoring a checkpoint would ignore missing variables.')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "# =========================================================================== #\n",
    "# Main training routine.\n",
    "# =========================================================================== #\n",
    "def main(_):\n",
    "    if not FLAGS.dataset_dir:\n",
    "        raise ValueError('You must supply the dataset directory with --dataset_dir')\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "    with tf.Graph().as_default():\n",
    "        # Config model_deploy. Keep TF Slim Models structure.\n",
    "        # Useful if want to need multiple GPUs and/or servers in the future.\n",
    "        deploy_config = model_deploy.DeploymentConfig(\n",
    "            num_clones=FLAGS.num_clones,\n",
    "            clone_on_cpu=FLAGS.clone_on_cpu,\n",
    "            replica_id=0,\n",
    "            num_replicas=1,\n",
    "            num_ps_tasks=0)\n",
    "        # Create global_step.\n",
    "        with tf.device(deploy_config.variables_device()):\n",
    "            global_step = slim.create_global_step()\n",
    "\n",
    "        # Select the dataset.\n",
    "        dataset = dataset_factory.get_dataset(\n",
    "            FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n",
    "\n",
    "        # Get the SSD network and its anchors.\n",
    "        ssd_class = nets_factory.get_network(FLAGS.model_name)\n",
    "        ssd_params = ssd_class.default_params._replace(num_classes=FLAGS.num_classes)\n",
    "        ssd_net = ssd_class(ssd_params)\n",
    "        ssd_shape = ssd_net.params.img_shape\n",
    "        ssd_anchors = ssd_net.anchors(ssd_shape)\n",
    "\n",
    "        # Select the preprocessing function.\n",
    "        preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n",
    "        image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n",
    "            preprocessing_name, is_training=True)\n",
    "\n",
    "        tf_utils.print_configuration(FLAGS.__flags, ssd_params,\n",
    "                                     dataset.data_sources, FLAGS.train_dir)\n",
    "        # =================================================================== #\n",
    "        # Create a dataset provider and batches.\n",
    "        # =================================================================== #\n",
    "        with tf.device(deploy_config.inputs_device()):\n",
    "            with tf.name_scope(FLAGS.dataset_name + '_data_provider'):\n",
    "                provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "                    dataset,\n",
    "                    num_readers=FLAGS.num_readers,\n",
    "                    common_queue_capacity=20 * FLAGS.batch_size,\n",
    "                    common_queue_min=10 * FLAGS.batch_size,\n",
    "                    shuffle=True)\n",
    "            # Get for SSD network: image, labels, bboxes.\n",
    "            [image, shape, glabels, gbboxes] = provider.get(['image', 'shape',\n",
    "                                                             'object/label',\n",
    "                                                             'object/bbox'])\n",
    "            # Pre-processing image, labels and bboxes.\n",
    "            image, glabels, gbboxes = \\\n",
    "                image_preprocessing_fn(image, glabels, gbboxes,\n",
    "                                       out_shape=ssd_shape,\n",
    "                                       data_format=DATA_FORMAT)\n",
    "            # Encode groundtruth labels and bboxes.\n",
    "            gclasses, glocalisations, gscores = \\\n",
    "                ssd_net.bboxes_encode(glabels, gbboxes, ssd_anchors)\n",
    "            batch_shape = [1] + [len(ssd_anchors)] * 3\n",
    "\n",
    "            # Training batches and queue.\n",
    "            r = tf.train.batch(\n",
    "                tf_utils.reshape_list([image, gclasses, glocalisations, gscores]),\n",
    "                batch_size=FLAGS.batch_size,\n",
    "                num_threads=FLAGS.num_preprocessing_threads,\n",
    "                capacity=5 * FLAGS.batch_size)\n",
    "            b_image, b_gclasses, b_glocalisations, b_gscores = \\\n",
    "                tf_utils.reshape_list(r, batch_shape)\n",
    "\n",
    "            # Intermediate queueing: unique batch computation pipeline for all\n",
    "            # GPUs running the training.\n",
    "            batch_queue = slim.prefetch_queue.prefetch_queue(\n",
    "                tf_utils.reshape_list([b_image, b_gclasses, b_glocalisations, b_gscores]),\n",
    "                capacity=2 * deploy_config.num_clones)\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Define the model running on every GPU.\n",
    "        # =================================================================== #\n",
    "        def clone_fn(batch_queue):\n",
    "            \"\"\"Allows data parallelism by creating multiple\n",
    "            clones of network_fn.\"\"\"\n",
    "            # Dequeue batch.\n",
    "            b_image, b_gclasses, b_glocalisations, b_gscores = \\\n",
    "                tf_utils.reshape_list(batch_queue.dequeue(), batch_shape)\n",
    "\n",
    "            # Construct SSD network.\n",
    "            arg_scope = ssd_net.arg_scope(weight_decay=FLAGS.weight_decay,\n",
    "                                          data_format=DATA_FORMAT)\n",
    "            with slim.arg_scope(arg_scope):\n",
    "                predictions, localisations, logits, end_points = \\\n",
    "                    ssd_net.net(b_image, is_training=True)\n",
    "            # Add loss function.\n",
    "            ssd_net.losses(logits, localisations,\n",
    "                           b_gclasses, b_glocalisations, b_gscores,\n",
    "                           match_threshold=FLAGS.match_threshold,\n",
    "                           negative_ratio=FLAGS.negative_ratio,\n",
    "                           alpha=FLAGS.loss_alpha,\n",
    "                           label_smoothing=FLAGS.label_smoothing)\n",
    "            return end_points\n",
    "\n",
    "        # Gather initial summaries.\n",
    "        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Add summaries from first clone.\n",
    "        # =================================================================== #\n",
    "        clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n",
    "        first_clone_scope = deploy_config.clone_scope(0)\n",
    "        # Gather update_ops from the first clone. These contain, for example,\n",
    "        # the updates for the batch_norm variables created by network_fn.\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n",
    "\n",
    "        # Add summaries for end_points.\n",
    "        end_points = clones[0].outputs\n",
    "        for end_point in end_points:\n",
    "            x = end_points[end_point]\n",
    "            summaries.add(tf.summary.histogram('activations/' + end_point, x))\n",
    "            summaries.add(tf.summary.scalar('sparsity/' + end_point,\n",
    "                                            tf.nn.zero_fraction(x)))\n",
    "        # Add summaries for losses and extra losses.\n",
    "        for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n",
    "            summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "        for loss in tf.get_collection('EXTRA_LOSSES', first_clone_scope):\n",
    "            summaries.add(tf.summary.scalar(loss.op.name, loss))\n",
    "\n",
    "        # Add summaries for variables.\n",
    "        for variable in slim.get_model_variables():\n",
    "            summaries.add(tf.summary.histogram(variable.op.name, variable))\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Configure the moving averages.\n",
    "        # =================================================================== #\n",
    "        if FLAGS.moving_average_decay:\n",
    "            moving_average_variables = slim.get_model_variables()\n",
    "            variable_averages = tf.train.ExponentialMovingAverage(\n",
    "                FLAGS.moving_average_decay, global_step)\n",
    "        else:\n",
    "            moving_average_variables, variable_averages = None, None\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Configure the optimization procedure.\n",
    "        # =================================================================== #\n",
    "        with tf.device(deploy_config.optimizer_device()):\n",
    "            learning_rate = tf_utils.configure_learning_rate(FLAGS,\n",
    "                                                             dataset.num_samples,\n",
    "                                                             global_step)\n",
    "            optimizer = tf_utils.configure_optimizer(FLAGS, learning_rate)\n",
    "            summaries.add(tf.summary.scalar('learning_rate', learning_rate))\n",
    "\n",
    "        if FLAGS.moving_average_decay:\n",
    "            # Update ops executed locally by trainer.\n",
    "            update_ops.append(variable_averages.apply(moving_average_variables))\n",
    "\n",
    "        # Variables to train.\n",
    "        variables_to_train = tf_utils.get_variables_to_train(FLAGS)\n",
    "\n",
    "        # and returns a train_tensor and summary_op\n",
    "        total_loss, clones_gradients = model_deploy.optimize_clones(\n",
    "            clones,\n",
    "            optimizer,\n",
    "            var_list=variables_to_train)\n",
    "        # Add total_loss to summary.\n",
    "        summaries.add(tf.summary.scalar('total_loss', total_loss))\n",
    "\n",
    "        # Create gradient updates.\n",
    "        grad_updates = optimizer.apply_gradients(clones_gradients,\n",
    "                                                 global_step=global_step)\n",
    "        update_ops.append(grad_updates)\n",
    "        update_op = tf.group(*update_ops)\n",
    "        train_tensor = control_flow_ops.with_dependencies([update_op], total_loss,\n",
    "                                                          name='train_op')\n",
    "\n",
    "        # Add the summaries from the first clone. These contain the summaries\n",
    "        summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n",
    "                                           first_clone_scope))\n",
    "        # Merge all summaries together.\n",
    "        summary_op = tf.summary.merge(list(summaries), name='summary_op')\n",
    "\n",
    "        # =================================================================== #\n",
    "        # Kicks off the training.\n",
    "        # =================================================================== #\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=FLAGS.gpu_memory_fraction)\n",
    "        config = tf.ConfigProto(log_device_placement=False,\n",
    "                                gpu_options=gpu_options)\n",
    "        saver = tf.train.Saver(max_to_keep=5,\n",
    "                               keep_checkpoint_every_n_hours=1.0,\n",
    "                               write_version=2,\n",
    "                               pad_step_number=False)\n",
    "        slim.learning.train(\n",
    "            train_tensor,\n",
    "            logdir=FLAGS.train_dir,\n",
    "            master='',\n",
    "            is_chief=True,\n",
    "            init_fn=tf_utils.get_init_fn(FLAGS),\n",
    "            summary_op=summary_op,\n",
    "            number_of_steps=FLAGS.max_number_of_steps,\n",
    "            log_every_n_steps=FLAGS.log_every_n_steps,\n",
    "            save_summaries_secs=FLAGS.save_summaries_secs,\n",
    "            saver=saver,\n",
    "            save_interval_secs=FLAGS.save_interval_secs,\n",
    "            session_config=config,\n",
    "            sync_optimizer=None)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-venv",
   "language": "python",
   "name": "tf-gpu-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
